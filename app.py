import os
from collections import defaultdict

import streamlit as st
from dotenv import load_dotenv

from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings

from search_timeline import (
    generate_timeline_synthesis,
    search_keyword_timeline,
    summarize_yearly_insights,
)

# ========================================
# ê¸°ë³¸ ì„¤ì •
# ========================================
load_dotenv()
groq_key = os.getenv("GROQ_API_KEY")

if not groq_key:
    st.error("âŒ GROQ_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .env ë˜ëŠ” Streamlit Secretsì— ë“±ë¡í•´ì£¼ì„¸ìš”.")
    st.stop()


# ========================================
# ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
# ========================================
@st.cache_resource
def load_vectorstore():
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    return FAISS.load_local(
        "faiss_index", embeddings, allow_dangerous_deserialization=True
    )


# ========================================
# LLM ë¡œë”©
# ========================================
@st.cache_resource
def load_llm():
    return ChatGroq(
        model_name="llama-3.1-8b-instant",
        temperature=0.1,
        groq_api_key=groq_key,
    )


@st.cache_resource
def load_bm25_retriever(_vs: FAISS):
    # ë²¡í„°ìŠ¤í† ì–´ ì•ˆì— ìˆëŠ” ì „ì²´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ BM25 ì¸ë±ìŠ¤ ìƒì„±
    all_docs = list(_vs.docstore._dict.values())
    # këŠ” í•œ ë²ˆì— ë°˜í™˜í•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (ì—¬ìœ  ìˆê²Œ ì„¤ì •)
    return BM25Retriever.from_documents(all_docs, k=50)


vectorstore = load_vectorstore()
bm25_retriever = load_bm25_retriever(vectorstore)
llm = load_llm()

# ê¸°ë³¸ retrieverëŠ” kë¥¼ ì¡°ê¸ˆ ë„‰ë„‰í•˜ê²Œ
retriever = vectorstore.as_retriever(search_kwargs={"k": 15})

CHAPTER_LABELS = ["Global Economy", "Consumer Shifts", "Fashion System"]


# ========================================
# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜ (semantic + BM25)
# ========================================
def hybrid_search(
    query: str,
    semantic_k: int = 30,
    keyword_k: int = 30,
    combined_k: int = 12,
    chapter_filter: str | None = None,
    region_filter: str | None = None,
):
    """
    - semantic: FAISS similarity_search
    - keyword: BM25Retriever
    ë‘ ê²°ê³¼ì˜ rankë¥¼ ì ìˆ˜ë¡œ ë³€í™˜í•´ì„œ ê°€ì¤‘ í‰ê·  í›„ ì¬ì •ë ¬.
    """
    semantic_docs = vectorstore.similarity_search(query, k=semantic_k)
    # ìµœì‹  BM25RetrieverëŠ” get_relevant_documents ëŒ€ì‹  invoke ì‚¬ìš©
    keyword_docs = bm25_retriever.invoke(query)[:keyword_k]

    def make_key(doc):
        return (
            doc.metadata.get("source"),
            doc.metadata.get("page"),
            doc.page_content,
        )

    scores = {}
    n_sem = len(semantic_docs) or 1
    n_kw = len(keyword_docs) or 1

    # semantic rank ê¸°ë°˜ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ê²Œ)
    for rank, doc in enumerate(semantic_docs):
        key = make_key(doc)
        sem_score = (n_sem - rank) / n_sem
        prev_sem, prev_kw, prev_doc = scores.get(key, (0.0, 0.0, doc))
        scores[key] = (max(prev_sem, sem_score), prev_kw, doc)

    # BM25 rank ê¸°ë°˜ ì ìˆ˜
    for rank, doc in enumerate(keyword_docs):
        key = make_key(doc)
        kw_score = (n_kw - rank) / n_kw
        prev_sem, prev_kw, prev_doc = scores.get(key, (0.0, 0.0, doc))
        scores[key] = (prev_sem, max(prev_kw, kw_score), doc)

    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ìƒì„±
    alpha = 0.6  # semantic ë¹„ì¤‘
    scored_docs = []
    for sem_score, kw_score, doc in scores.values():
        final_score = alpha * sem_score + (1 - alpha) * kw_score

        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
        if chapter_filter and doc.metadata.get("chapter") != chapter_filter:
            continue
        if region_filter and doc.metadata.get("region") != region_filter:
            continue

        scored_docs.append((final_score, doc))

    # í•„í„°ë§ í›„ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•„í„° ì—†ì´ fallback
    if not scored_docs:
        scored_docs = [
            (
                alpha * ((n_sem - i) / n_sem),
                d,
            )
            for i, d in enumerate(semantic_docs)
        ]

    scored_docs.sort(key=lambda x: x[0], reverse=True)
    return [d for _, d in scored_docs[:combined_k]]


# ========================================
# ë¬¸ì„œ ê·¸ë£¹ ë¡œë”©
# ========================================
@st.cache_resource
def load_grouped_docs():
    all_docs = list(vectorstore.docstore._dict.values())
    by_year_chapter = defaultdict(list)
    by_chapter = defaultdict(list)

    for d in all_docs:
        year = d.metadata.get("year")
        chapter = d.metadata.get("chapter")
        by_year_chapter[(year, chapter)].append(d)
        by_chapter[chapter].append(d)

    return by_year_chapter, by_chapter


by_year_chapter, by_chapter = load_grouped_docs()


# ========================================
# í—¬í¼: ë¬¸ì„œ í¬ë§·íŒ…
# ========================================
def format_docs(docs):
    processed = []
    for d in docs:
        src = os.path.basename(d.metadata.get("source", ""))
        page = d.metadata.get("page", "?")
        year = d.metadata.get("year", "")
        chapter = d.metadata.get("chapter", "")
        region = d.metadata.get("region", "")
        if region:
            header = f"[{year} / {chapter} / {region} / {src} p.{page}]"
        else:
            header = f"[{year} / {chapter} / {src} p.{page}]"
        processed.append(header + "\n" + d.page_content)
    return "\n\n".join(processed)


# ========================================
# ê³µí†µ RAG í”„ë¡¬í”„íŠ¸
# ========================================
qa_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a professional Fashion MD Research Assistant.\n"
            "Use ONLY the content from McKinsey & BoF 'State of Fashion' (2021â€“2025).\n"
            "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, í•µì‹¬ ìš©ì–´ëŠ” ì˜ì–´ ë³‘ê¸°í•´ì¤˜.",
        ),
        (
            "human",
            "ì§ˆë¬¸: {question}\n\n"
            "ì°¸ê³  ë¬¸ì„œ:\n{context}",
        ),
    ]
)

qa_chain = qa_prompt | llm | StrOutputParser()


# ========================================
# ëŒ€í™” ë¡œê·¸ ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
# ========================================
report_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a senior fashion strategy consultant.\n"
            "Below is a conversation between a Fashion MD and an AI research assistant\n"
            "about insights from McKinsey & BoF 'State of Fashion' (2021â€“2025).\n"
            "Use ONLY information that can be reasonably grounded in this conversation.\n"
            "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , í•µì‹¬ ê°œë…ì€ í•„ìš”í•  ë•Œë§Œ ì˜ì–´ ë³‘ê¸°í•´ì¤˜.",
        ),
        (
            "human",
            "ë‹¤ìŒì€ ì‚¬ìš©ì(íŒ¨ì…˜ MD)ì™€ AI ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ëŒ€í™” ë¡œê·¸ì…ë‹ˆë‹¤.\n"
            "ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
            "ëŒ€í™” ë¡œê·¸:\n{conversation}\n\n"
            "ğŸ“Œ ë¦¬í¬íŠ¸ êµ¬ì„±ì€ ë‹¤ìŒ ì„¹ì…˜ì„ í¬í•¨í•´ ì£¼ì„¸ìš”.\n"
            "1. Executive Summary\n"
            "2. Key Insights (bullet í˜•íƒœ)\n"
            "3. Implications & Action Ideas (í˜„ì—… í™œìš© ì•„ì´ë””ì–´ ì¤‘ì‹¬)\n\n"
            "âš ï¸ ì£¼ì˜ì‚¬í•­\n"
            "- ë°˜ë“œì‹œ ëŒ€í™” ë‚´ìš©ì—ì„œ íŒŒìƒë  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë§Œ ì •ë¦¬í•  ê²ƒ\n"
            "- McKinsey/BoF ë¦¬í¬íŠ¸ì— ì¼ë°˜ì ìœ¼ë¡œ ë“±ì¥í•  ë²•í•œ ë¬¸ì¥ì´ë¼ë„, ëŒ€í™”ì— ì „í˜€ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤ë©´ ìƒì„±í•˜ì§€ ë§ ê²ƒ\n"
            "- í•œêµ­ì–´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ë˜, í•„ìš”í•œ í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°\n"
            "- ë¬¸ì¥ì€ ì§§ê³  ëª…ë£Œí•˜ê²Œ, ì‹¤ì œ ë³´ê³ ì„œì— ë°”ë¡œ ë¶™ì—¬ ë„£ì„ ìˆ˜ ìˆëŠ” í†¤ìœ¼ë¡œ ì‘ì„±",
        ),
    ]
)

report_chain = report_prompt | llm | StrOutputParser()


# ========================================
# Streamlit UI ì‹œì‘
# ========================================
st.set_page_config(page_title="State of Fashion â€” AI Insight Engine")

st.title("The State of Fashion")
st.title("- AI Insight Engine")
st.caption("AI-powered Insight from SoF 2021â€“2025 Reports")

st.markdown("---")

# ========================================
# ë©”ì¸ íƒ­ êµ¬ì„±
# ========================================
tab_main, tab_keyword, tab_chapter, tab_country, tab_chat = st.tabs([
    "1ï¸âƒ£ AI Report Search",
    "2ï¸âƒ£ Keyword Analytics",
    "3ï¸âƒ£ Chapter Insights",
    "4ï¸âƒ£ Regional Insights",
    "5ï¸âƒ£ Chat & Report",
])


# ============================================================================
# ğŸ“Œ TAB 1 â€” ì „ì²´ ê²€ìƒ‰ & ì§ˆë¬¸í•˜ê¸°
# ============================================================================
with tab_main:
    st.subheader("Ask Anything â€” AI Analyzes the Report to Answer Your Questions")

    question = st.text_area("ì§ˆë¬¸ ì…ë ¥", key="qa_question")
    chapter_filter = st.selectbox(
        "ê²€ìƒ‰í•  ì±•í„° (ì˜µì…˜)", ["ì „ì²´"] + CHAPTER_LABELS, index=0
    )

    if st.button("AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°", key="qa_button"):
        if not question.strip():
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                ch = None if chapter_filter == "ì „ì²´" else chapter_filter

                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ë¬¸ì„œ ê²€ìƒ‰
                docs = hybrid_search(
                    question,
                    semantic_k=30,
                    keyword_k=30,
                    combined_k=12,
                    chapter_filter=ch,
                )

                # LLM ì»¨í…ìŠ¤íŠ¸ëŠ” ìƒìœ„ 8ê°œ ì •ë„ë§Œ ì‚¬ìš©
                context = format_docs(docs[:8])
                answer = qa_chain.invoke({"question": question, "context": context})

            st.markdown("### ğŸ“Œ ë‹µë³€")
            st.write(answer)

            # -----------------------
            # RAG Validation Snippets
            # -----------------------
            st.markdown("### ğŸ” ì°¸ê³  ë¬¸ì¥ (Top 3)")
            if not docs:
                st.info("ì°¸ê³ í•  ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                for i, d in enumerate(docs[:3], start=1):
                    src = os.path.basename(d.metadata.get("source", ""))
                    page = d.metadata.get("page", "?")
                    year = d.metadata.get("year", "")
                    chapter = d.metadata.get("chapter", "")
                    region = d.metadata.get("region", "")

                    meta_line = f"{year} / {chapter}"
                    if region:
                        meta_line += f" / {region}"
                    meta_line += f" / {src} p.{page}"

                    st.markdown(f"**[{i}] {meta_line}**")
                    st.write(d.page_content)
                    st.markdown("---")


# ============================================================================
# ğŸ“Œ TAB 2 â€” Chapter Insight (ì„œë¸Œíƒ­ 4ê°œ)
# ============================================================================
with tab_chapter:

    sub1, sub2, sub3 = st.tabs(
        [
            "Annual Keyword Insights",
            "Chapter Keyword Timeline",
            "Keyword Mapping"
        ]
    )

    # ---------------------------------------------------
    # ğŸ“Œ ì„œë¸Œíƒ­ 1 â€” ì—°ë„ë³„ í•µì‹¬ í‚¤ì›Œë“œ
    # ---------------------------------------------------
    with sub1:
        st.subheader("Key Keywords by Year")

        col1, col2 = st.columns(2)
        with col1:
            year = st.selectbox("ì—°ë„ ì„ íƒ", [2021, 2022, 2023, 2024, 2025])
        with col2:
            chapter = st.selectbox("ì±•í„° ì„ íƒ", CHAPTER_LABELS)

        if st.button("í‚¤ì›Œë“œ ìƒì„±", key="year_chapter_summary_keywords"):
            key = (year, chapter)
            docs = by_year_chapter.get(key, [])

            if not docs:
                st.warning("í•´ë‹¹ ì—°ë„/ì±•í„°ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                text = "\n\n".join(d.page_content for d in docs[:20])

                summary_prompt = ChatPromptTemplate.from_messages(
                    [
                        (
                            "system",
                            "You are a senior fashion strategy analyst. "
                            "ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ì—°ë„/ì±•í„°ì˜ í•µì‹¬ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ 5ê°œ ë½‘ì•„ "
                            "ê° í‚¤ì›Œë“œë‹¹ 1~2ë¬¸ì¥ ì„¤ëª…ì„ ë§Œë“¤ì–´ì¤˜.\n"
                            "ì„¤ëª…ì€ í•œêµ­ì–´ë¡œ, ì¤‘ìš”í•œ ìš©ì–´ëŠ” ì˜ì–´ ë³‘ê¸°í•´ì¤˜."
                        ),
                        (
                            "human",
                            "ì—°ë„: {year}\nì±•í„°: {chapter}\n\n"
                            "ë¶„ì„ í…ìŠ¤íŠ¸:\n{text}\n\n"
                            "â¡ ì¶œë ¥ í˜•ì‹:\n"
                            "Key Insights\n"
                            "- í‚¤ì›Œë“œ 1: ì„¤ëª…(1~2ì¤„)\n"
                            "- í‚¤ì›Œë“œ 2: ì„¤ëª…\n"
                            "- í‚¤ì›Œë“œ 3: ì„¤ëª…\n"
                            "- í‚¤ì›Œë“œ 4: ì„¤ëª…\n"
                            "- í‚¤ì›Œë“œ 5: ì„¤ëª…"
                        ),
                    ]
                )

                chain = summary_prompt | llm | StrOutputParser()

                with st.spinner("í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                    summary = chain.invoke(
                        {"year": year, "chapter": chapter, "text": text}
                    )

                st.write(summary)


    # ---------------------------------------------------
    # ğŸ“Œ ì„œë¸Œíƒ­ 2 â€” ì±•í„°ë³„ í‚¤ì›Œë“œ íƒ€ì„ë¼ì¸
    # ---------------------------------------------------
    with sub2:
        st.subheader("Chapter-Based Keyword Timeline Analysis")

        keyword = st.text_input(
            "ë¶„ì„í•  í‚¤ì›Œë“œ (ì˜ˆ: AI, resale, sustainability, Gen Z, silver spenders...)", key="timeline_keyword"
        )
        chapter_sel = st.selectbox(
            "ì±•í„° ì„ íƒ", ["ì „ì²´"] + CHAPTER_LABELS, index=0, key="timeline_chapter"
        )

        if st.button("íƒ€ì„ë¼ì¸ ìƒì„±", key="timeline_button"):
            if not keyword.strip():
                st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                ch = None if chapter_sel == "ì „ì²´" else chapter_sel

                with st.spinner("íƒ€ì„ë¼ì¸ ë¶„ì„ ì¤‘..."):
                    grouped = search_keyword_timeline(keyword, retriever, chapter=ch)

                    timeline_full = {yr: grouped.get(yr, []) for yr in [2021, 2022, 2023, 2024, 2025]}

                    yearly_summary = {}
                    for yr, docs in timeline_full.items():

                        if not docs:
                            yearly_summary[yr] = "âš ï¸ í•´ë‹¹ ì—°ë„ì—ì„œëŠ” í‚¤ì›Œë“œ ì–¸ê¸‰ì´ ê±°ì˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
                        else:
                            text = "\n\n".join(docs[:3])
                            prompt = ChatPromptTemplate.from_messages(
                                [
                                    (
                                        "system",
                                        "You are a fashion trend analyst. "
                                        "ì•„ë˜ í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ í•´ë‹¹ ì—°ë„ì˜ ê´€ì ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.\n"
                                        "â— ì ˆëŒ€ ê¸ˆì§€:\n"
                                        "- '2023ë…„ì˜ í‚¤ì›Œë“œëŠ” ~ì…ë‹ˆë‹¤' ê°™ì€ ë¬¸ì¥ ìƒì„±\n"
                                        "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ëŒ€í‘œ í‚¤ì›Œë“œ ìƒì„±\n"
                                        "- íŒ¨ì…˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì„ ì–¸\n"
                                        "- í•´ì„ ì§€ì–´ë‚´ê¸°\n"
                                        "â— ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                                        "- í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš”ì•½ë§Œ ìƒì„±\n"
                                        "- í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ë˜ í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°"
                                    ),
                                    (
                                        "human",
                                        "í‚¤ì›Œë“œ: {keyword}\nì—°ë„: {year}\n\ní…ìŠ¤íŠ¸:\n{text}"
                                    ),
                                ]
                            )
                            chain = prompt | llm | StrOutputParser()
                            summary = chain.invoke({"keyword": keyword, "year": yr, "text": text})
                            yearly_summary[yr] = summary

                    synthesis_prompt = ChatPromptTemplate.from_messages(
                        [
                            (
                                "system",
                                "You are a senior fashion strategist."
                                "ì—°ë„ë³„ ë¶„ì„ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ íë¦„ì„ ë”± 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.\n"
                                "â— ì ˆëŒ€ ê¸ˆì§€:\n"
                                "- 'ì „ì²´ í‚¤ì›Œë“œëŠ” ~ì…ë‹ˆë‹¤' ë¬¸ì¥ ìƒì„±\n"
                                "- ëŒ€í‘œ í‚¤ì›Œë“œ ì„ ì–¸\n"
                                "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ê°œë… ì¶”ê°€\n"
                                "â— ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                                "- ìì—°ìŠ¤ëŸ¬ìš´ 3ë¬¸ì¥ ìš”ì•½ë§Œ ìƒì„±"
                            ),
                            (
                                "human",
                                "í‚¤ì›Œë“œ: {keyword}\n\nì—°ë„ë³„ ë‚´ìš©:\n{summary}"
                            ),
                        ]
                    )

                    combined = "\n".join(f"[{yr}] {txt}" for yr, txt in yearly_summary.items())
                    chain = synthesis_prompt | llm | StrOutputParser()
                    synthesis = chain.invoke({"keyword": keyword, "summary": combined})

                st.subheader(f"í‚¤ì›Œë“œ íƒ€ì„ë¼ì¸ : **{keyword}**")

                for yr in [2021, 2022, 2023, 2024, 2025]:
                    st.write(f"### ğŸ“Œ {yr}ë…„")
                    st.write(yearly_summary[yr])
                    st.markdown("---")

                st.write("### ì „ì²´ íë¦„ ìš”ì•½")
                st.write(synthesis)


    # ---------------------------------------------------
    # ğŸ“Œ ì„œë¸Œíƒ­ 3 â€” í‚¤ì›Œë“œ Ã— ì±•í„° ë§¤í•‘
    # ---------------------------------------------------
    with sub3:
        st.subheader("Keyword Mapping Table")

        keyword_map = st.text_input(
            "í‚¤ì›Œë“œ ì…ë ¥ (ì˜ˆ: AI, resale, sustainability, Gen Z, silver spenders...)", key="mapping_keyword"
        )

        if st.button("ë§¤í•‘ ìƒì„±", key="mapping_button"):
            if not keyword_map.strip():
                st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                import pandas as pd

                rows = []

                with st.spinner("ë§¤í•‘ í…Œì´ë¸” ìƒì„± ì¤‘..."):
                    for ch in CHAPTER_LABELS:
                        grouped = search_keyword_timeline(keyword_map, retriever, chapter=ch)

                        # ğŸ“Œ ì±•í„° ë‚´ ê²€ìƒ‰ê²°ê³¼ ì—†ì„ ê²½ìš°
                        if not grouped:
                            rows.append({"Chapter": ch, "Perspective": "ê´€ë ¨ëœ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."})
                            continue

                        # ì—°ë„ë³„ ìš”ì•½
                        yearly = summarize_yearly_insights(grouped, keyword_map, chapter=ch)

                        # ì—°ë„ë³„ í…ìŠ¤íŠ¸ ì¡°í•©
                        combined = "\n\n".join(
                            f"[{y}]\n{txt}" for y, txt in sorted(yearly.items())
                        )

                        # ğŸ“Œ í•µì‹¬ ë¬¸ì¥ 3ë¬¸ì¥ë§Œ ìƒì„±í•˜ë„ë¡ ì œí•œí•˜ëŠ” í”„ë¡¬í”„íŠ¸
                        map_prompt = ChatPromptTemplate.from_messages(
                            [
                                (
                                    "system",
                                    "You are a fashion strategy analyst."
                                    "ì•„ë˜ ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ì±•í„°ê°€ ì´ í‚¤ì›Œë“œë¥¼ ì–´ë–»ê²Œ ë‹¤ë£¨ëŠ”ì§€ í•µì‹¬ 3ë¬¸ì¥ìœ¼ë¡œë§Œ ì •ë¦¬í•´ì¤˜\n"
                                    "âš ï¸ ì ˆëŒ€ ê¸ˆì§€:\n"
                                    "- 'í‚¤ì›Œë“œ: ~' í˜•ì‹ ë¬¸ì¥ ìƒì„± ê¸ˆì§€\n"
                                    "- '202Xë…„ ~ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤' ê¸ˆì§€\n"
                                    "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ìˆ«ì/ì‚¬ì‹¤/í‚¤ì›Œë“œ ìƒì„± ê¸ˆì§€\n"
                                    "âš ï¸ ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                                    "- í…ìŠ¤íŠ¸ ê¸°ë°˜ í•µì‹¬ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ¬ìš´ 3ë¬¸ì¥ìœ¼ë¡œë§Œ ìš”ì•½\n"
                                    "- í•œêµ­ì–´ë¡œ ì„œìˆ , í•„ìš”í•œ ê²½ìš° í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°"
                                ),
                                (
                                    "human",
                                    "í‚¤ì›Œë“œ: {keyword}\nì±•í„°: {chapter}\n\n"
                                    "ìš”ì•½ í…ìŠ¤íŠ¸:\n{summary}"
                                ),
                            ]
                        )

                        chain = map_prompt | llm | StrOutputParser()

                        perspective = chain.invoke(
                            {
                                "keyword": keyword_map,
                                "chapter": ch,
                                "summary": combined,
                            }
                        )

                        rows.append({"Chapter": ch, "Perspective": perspective})

                df = pd.DataFrame(rows)
                st.table(df)

# =====================================================================
# ğŸ“Œ TAB 2 â€” êµ­ê°€ë³„ ì¸ì‚¬ì´íŠ¸
# =====================================================================
with tab_country:

    st.subheader("ğŸŒ Regional Market Insights (2024 & 2025)")

    country = st.selectbox(
        "êµ­ê°€ ì„ íƒ",
        ["ğŸ‡¯ğŸ‡µ Japan", "ğŸ‡®ğŸ‡³ India", "ğŸ‡ºğŸ‡¸ US", "ğŸ‡¨ğŸ‡³ China", "ğŸ‡ªğŸ‡º EU"],
        index=0,
    )

    # êµ­ê°€ëª…ì„ AIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    country_map = {
        "ğŸ‡¯ğŸ‡µ Japan": "Japan",
        "ğŸ‡®ğŸ‡³ India": "India",
        "ğŸ‡ºğŸ‡¸ US": "United States",
        "ğŸ‡¨ğŸ‡³ China": "China",
        "ğŸ‡ªğŸ‡º EU": "European Union",
    }
    country_text = country_map[country]

    if st.button("êµ­ê°€ë³„ ì¸ì‚¬ì´íŠ¸ ìƒì„±", key="country_insight"):
        with st.spinner("êµ­ê°€ë³„ ì‹œì¥ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘..."):

            # 1) RAG ê²€ìƒ‰: êµ­ê°€ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§
            query = f"{country_text} market consumer trend economy fashion"

            # region ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            docs = hybrid_search(
                query,
                semantic_k=30,
                keyword_k=30,
                combined_k=25,
                region_filter=country_text,
            )

            # ì—°ë„ë³„ ë¶„ë¦¬
            docs_2025 = [d.page_content for d in docs if d.metadata.get("year") == 2025]
            docs_2024 = [d.page_content for d in docs if d.metadata.get("year") == 2024]

            def get_summary(texts, year):
                """LLMì„ ì´ìš©í•œ ì—°ë„ë³„ ìš”ì•½ í•¨ìˆ˜"""
                if not texts:
                    return f"âš ï¸ {year}ë…„ì—ëŠ” í•´ë‹¹ êµ­ê°€ ê´€ë ¨ ì •ë³´ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤."

                combined = "\n\n".join(texts[:8])  # ë„ˆë¬´ ê¸´ ê²½ìš° ì••ì¶•

                prompt = ChatPromptTemplate.from_messages(
                    [
                        (
                            "system",
                            "You are a senior global fashion strategist.\n"
                            "ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ êµ­ê°€ì˜ ì‹œì¥ íŠ¹ì„±ì„ ì •í™•í•˜ê²Œ 3ë¬¸ì¥ìœ¼ë¡œë§Œ ìš”ì•½í•˜ë¼.\n\n"
                            "âš ï¸ ì ˆëŒ€ ê¸ˆì§€:\n"
                            "- 'í•´ë‹¹ êµ­ê°€ì˜ ì‹œì¥ íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤' ê°™ì€ ì„œë¡  ë¬¸ì¥ ìƒì„± ê¸ˆì§€\n"
                            "- í‚¤ì›Œë“œ ì„ ì–¸(ì˜ˆ: '2025ë…„ì˜ í‚¤ì›Œë“œëŠ” ~ì´ë‹¤') ê¸ˆì§€\n\n"
                            "- 'í‚¤ì›Œë“œ: ~' í˜•ì‹ ê¸ˆì§€\n"
                            "- '202Xë…„ì˜ íŠ¸ë Œë“œëŠ” ~ì…ë‹ˆë‹¤' ê¸ˆì§€\n"
                            "- '~ì˜ ì‹œì¥ íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.' ê¸ˆì§€\n"
                            "- '~ì˜ ì‹œì¥ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.' ê¸ˆì§€\n"
                            "- ì™¸ë˜ ë¬¸ìÂ·ë¹„ìì—°ìŠ¤ëŸ¬ìš´ ì–´êµ¬ ìƒì„± ê¸ˆì§€\n"
                            "- í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì¶”ë¡ /ê°€ì •/ìˆ«ì ìƒì„± ê¸ˆì§€\n"
                            "- ì„œë¡ Â·ê²°ë¡ Â·ì¥ì‹ì  ë¬¸ì¥ ê¸ˆì§€\n\n"
                            "- ê²°ë¡ Â·ì¡°ì–¸ ë¬¸ì¥ ê¸ˆì§€\n"
                            "âš ï¸ ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n"
                            "- í…ìŠ¤íŠ¸ ê¸°ë°˜ í•µì‹¬ë§Œ 3ë¬¸ì¥\n"
                            "- í•œêµ­ì–´ë¡œ ìƒì„±, í•„ìš” ì‹œ í•µì‹¬ ìš©ì–´ë§Œ ì˜ì–´ ë³‘ê¸°"
                            "- ì˜¤ì§ í…ìŠ¤íŠ¸ì— ìˆëŠ” ì‚¬ì‹¤ë§Œ 3ê°œì˜ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬\n"
                            "- ì „ë¬¸ì ì¸ ë¬¸ì²´ ìœ ì§€, ë‹¨ë¬¸/êµ°ë”ë”ê¸° ì—†ëŠ” í‘œí˜„\n"
                            "- í•„ìš”í•œ ê²½ìš°ì—ë§Œ í•µì‹¬ ìš©ì–´ ì˜ì–´ ë³‘ê¸°"
                        ),
                        (
                            "human",
                            f"{year}ë…„ì˜ '{country_text}' ê´€ë ¨ í…ìŠ¤íŠ¸:\n\n{combined}"
                        ),
                    ]
                )

                chain = prompt | llm | StrOutputParser()
                return chain.invoke({})

            summary_2025 = get_summary(docs_2025, 2025)
            summary_2024 = get_summary(docs_2024, 2024)

        # ì¶œë ¥ UI
        st.markdown(f"### ğŸŒ {country_text} â€” Market Insights")

        st.write("### ğŸ“Œ 2025ë…„")
        st.write(summary_2025)
        st.markdown("---")

        st.write("### ğŸ“Œ 2024ë…„")
        st.write(summary_2024)

# =====================================================================
# ğŸ“Œ TAB â€” í‚¤ì›Œë“œ ì‹œê°í™” (Top 10 Bar + Top3 Line Chart)
# =====================================================================
with tab_keyword:

    st.subheader("Top 10 Keywords")

    import re
    from collections import Counter
    import pandas as pd
    import plotly.express as px

    # ---------------------------
    # (A) ê°•í™”ëœ í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜
    # ---------------------------
    def extract_keywords(text):
        tokens = re.findall(r"[A-Za-z][A-Za-z\-]+", text)
        tokens = [t.lower() for t in tokens if len(t) > 3]

        stopwords = {
            # ì¼ë°˜ ì˜ì–´ ë¶ˆìš©ì–´
            "that","with","this","have","from","will","into","been","more","than",
            "their","which","also","about","what","when","were","your","them","they",
            "over","only","some","make","made","like","just","very","those","while",
            "where","such","many","each","most","much","other","would","should",
            "could","might","these","both","through","across","there","after","before",
            "under","between","because","based","during","within","without","using",
            "over","well","however","even","though","still","every","including",

            # ìˆ«ì í‘œí˜„
            "percent","million","billion","thousand",

            # íŒ¨ì…˜ ë¬¸ì„œì—ì„œ ë„ˆë¬´ ê¸°ë³¸ì ì¸ ë‹¨ì–´ë“¤
            "brands","brand","business","market","industry","consumer","consumers","customer",
            "customers","global","fashion","system","trend","analysis","report",
            "state","chapter","growth","people","products","product","value",
            "goods","retail","sales","year","years","company","companies",

            # ë¶ˆí•„ìš” í† í°
            "said","https","http","mckinsey",
        }

        tokens = [t for t in tokens if t not in stopwords]

        # ì¶”ê°€ í•„í„°ë§
        tokens = [t for t in tokens if not t.endswith("ing")]     # ë™ëª…ì‚¬ ì œê±°
        tokens = [t for t in tokens if len(set(t)) > 2]           # ë°˜ë³µ ë¬¸ì ì œê±°

        return tokens

    # ---------------------------
    # (B) ì—°ë„ë³„ í…ìŠ¤íŠ¸ ì·¨í•©
    # ---------------------------
    year_texts = {year: "" for year in [2021, 2022, 2023, 2024, 2025]}
    all_docs = list(vectorstore.docstore._dict.values())

    for d in all_docs:
        y = d.metadata.get("year")
        if y in year_texts:
            year_texts[y] += " " + d.page_content

    yearly_keyword_counts = {
        year: Counter(extract_keywords(text))
        for year, text in year_texts.items()
    }

    # ---------------------------
    # (C) ì—°ë„ ì„ íƒ UI
    # ---------------------------
    selected_year = st.selectbox(
        "ì—°ë„ ì„ íƒ",
        [2021, 2022, 2023, 2024, 2025],
        key="keyword_visual_year"
    )

    st.markdown("---")

    # ---------------------------
    # (D) Bar Chart ì¶œë ¥
    # ---------------------------

    top_keywords = yearly_keyword_counts[selected_year].most_common(10)

    if not top_keywords:
        st.warning("í•´ë‹¹ ì—°ë„ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    df_bar = pd.DataFrame({
        "keyword": [k for k, _ in top_keywords],
        "count": [v for _, v in top_keywords],
    })

    fig = px.bar(
        df_bar,
        x="keyword",
        y="count",
        title=f"{selected_year} Keyword Top 10",
        color="count",
        color_continuous_scale="Blues"
    )

    st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")
    st.write("Top 3 Keywords â€” Yearly Trend (2021â€“2025)")

    # ---------------------------
    # (E) ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ì„ íƒ
    # ---------------------------
    top3_keywords = [k for k, _ in top_keywords[:3]]

    # ---------------------------
    # (F) Top3 í‚¤ì›Œë“œë¥¼ ì—°ë„ë³„ë¡œ ë¹ˆë„ ê¸°ë°˜ ë³€í™” ê³„ì‚°
    # ---------------------------
    for keyword in top3_keywords:
        trend_counts = []
        for yr in [2021, 2022, 2023, 2024, 2025]:
            cnt = yearly_keyword_counts[yr][keyword]
            trend_counts.append(cnt)

        df_line = pd.DataFrame({
            "year": ["2021", "2022", "2023", "2024", "2025"],
            "count": trend_counts
        })

        df_line["year"] = df_line["year"].astype(str)

        st.write(f"ğŸ” {keyword}")

        fig_line = px.line(
            df_line,
            x="year",
            y="count",
            markers=True
        )

        fig_line.update_xaxes(type="category")
        st.plotly_chart(fig_line, use_container_width=True)
        st.markdown("---")


# =====================================================================
# ğŸ“Œ TAB 5 â€” ëŒ€í™”í˜• ì±—ë´‡ & ë¦¬í¬íŠ¸ ìƒì„±
# =====================================================================
with tab_chat:
    st.subheader("Conversational Strategy Copilot")
    st.caption("ì±—ë´‡ê³¼ ììœ ë¡­ê²Œ ëŒ€í™”í•œ ë’¤, ëŒ€í™” ë‚´ìš©ì„ ë¦¬í¬íŠ¸ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "chat_report" not in st.session_state:
        st.session_state.chat_report = ""

    # ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.chat_input("íŒ¨ì…˜Â·ë¦¬í…Œì¼ ì¸ì‚¬ì´íŠ¸ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

    if user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("AIê°€ SoF ë¦¬í¬íŠ¸ë¥¼ ì°¸ê³ í•´ ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
                docs = hybrid_search(
                    user_input,
                    semantic_k=30,
                    keyword_k=30,
                    combined_k=12,
                )
                context = format_docs(docs[:8])
                answer = qa_chain.invoke(
                    {"question": user_input, "context": context}
                )
                st.markdown(answer)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì €ì¥
        st.session_state.chat_history.append(
            {"role": "assistant", "content": answer}
        )

    st.markdown("---")
    st.markdown("### ğŸ“ ëŒ€í™” ë‚´ìš©ì„ ë¦¬í¬íŠ¸ë¡œ ì •ë¦¬í•˜ê¸°")

    col_report_btn, col_clear = st.columns([2, 1])

    with col_report_btn:
        generate_report = st.button("ëŒ€í™” ë‚´ìš©ìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±")
    with col_clear:
        clear_chat = st.button("ëŒ€í™” ë° ë¦¬í¬íŠ¸ ì´ˆê¸°í™”")

    if clear_chat:
        st.session_state.chat_history = []
        st.session_state.chat_report = ""
        st.experimental_rerun()

    if generate_report:
        if not st.session_state.chat_history:
            st.warning("ë¨¼ì € ì±—ë´‡ê³¼ ëª‡ ë²ˆ ëŒ€í™”ë¥¼ ë‚˜ëˆˆ ë’¤ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        else:
            # ëŒ€í™” ë¡œê·¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³‘í•©
            lines = []
            for msg in st.session_state.chat_history:
                role_label = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
                lines.append(f"{role_label}: {msg['content']}")

            conversation_text = "\n".join(lines)

            with st.spinner("ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½ ë¦¬í¬íŠ¸ë¡œ ì •ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                report = report_chain.invoke({"conversation": conversation_text})

            st.session_state.chat_report = report

    if st.session_state.chat_report:
        st.markdown("### ğŸ“„ Generated Conversation Report")
        st.write(st.session_state.chat_report)
